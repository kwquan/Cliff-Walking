{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walking\n",
    "### This notebook attempts to solve the Cliff Walking-V0 problem[part of OpenAI's gym environments] using Sarsa & Q-learning\n",
    "### Description of problem: \n",
    "### A 4x12 grid environment, where the agent[starting from bottom left grid][S] attempts to reach the goal[G] in the shortest time possible. All transition rewards=-1 except for stepping onto cliff. If agent steps onto cliff[states 37-46], it transits to initial position with reward=-100. Episode terminates when agent reaches goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating an array for storing q_values[q_values]. This has num_state rows[48] & num_action columns[4] \\\n",
    "We also create an optimal_policy array for storing optimal actions for each state after running all episodes.     \n",
    "Observe that this array only has 37 states[48-goal state-cliff states] \\\n",
    "Next, we set parameters[epsilon,discount,step_size,episodes] \\\n",
    "Finally, we create 2 empty lists, one for storing the reward of every timestep[total_rewards], the other for storing average rewards for every i%10==0 episode[average_rewards] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0', new_step_api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = np.zeros((48,4))\n",
    "optimal_policy = np.zeros(37)\n",
    "epsilon = 0.1\n",
    "discount = 0.9\n",
    "step_size = 0.5\n",
    "episodes = 100\n",
    "total_rewards = []\n",
    "average_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some useful functions for agent behavior \\\n",
    "1) choose_action(): \\\n",
    "This calculates action probability & selects an action. If action_prob < epsilon, it will take an exploratory action. Else, act greedily \\\n",
    "2) get_optimal_policy(): \\\n",
    "This obtains the optimal action for each state after running through all episodes. It loops through each state in optimal_policy and gets it's optimal action from q_values\n",
    "3) agent_step(): \\\n",
    "This updates the state-action pair value when next_state is non-terminal \\\n",
    "4) agent_end(): \\\n",
    "This updates the state-action pair value when next_state is terminal. Notice the difference between this & agent_step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(s):\n",
    "    action_prob = np.random.uniform(s)\n",
    "    if action_prob < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.random.choice([idx for idx in range(4) if q_values[s][idx] == max(q_values[s])])\n",
    "    return int(action)    \n",
    "    \n",
    "def get_optimal_policy():\n",
    "    for i in range(len(optimal_policy)):\n",
    "        optimal_policy[i] = np.random.choice([idx for idx in range(4) if q_values[i][idx] == max(q_values[i])])\n",
    "        \n",
    "def agent_step():\n",
    "    q_values[state][action] += step_size*(reward + discount*q_values[next_state][next_action] - q_values[state][action])\n",
    "    \n",
    "def agent_end():\n",
    "    q_values[state][action] += step_size*(reward - q_values[state][action])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic is as follows: \\\n",
    "For every episode, \\\n",
    "1) reset environment(env.reset()) \\\n",
    "2) initialise 1st action to None \\\n",
    "3) reset terminate to False \\\n",
    "\n",
    "For each timestep when terminate is False: \\\n",
    "1) choose action if it's 1st timestep \\\n",
    "2) pass action to env.step() to obtain next_state, reward & terminate \\\n",
    "3) if next_state is not terminal state: \\\n",
    "&ensp;    a) choose next_action \\\n",
    "&ensp;    b) run agent_step() to update action value \\\n",
    "&ensp;    c) set state to next_state \\\n",
    "&ensp;    d) set action to next_action \\\n",
    "4) if next_state is terminal:   \n",
    "&ensp;    a) run agent_end() to update action value \\\n",
    "5) append reward to total_rewards\n",
    "\n",
    "For every 10th episode, calculate average reward from total_rewards & append to average_rewards\n",
    "\n",
    "After running through all episodes, run get_optimal_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 832.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(episodes)):\n",
    "    state = env.reset()\n",
    "    action = None\n",
    "    terminate = False\n",
    "    while not terminate:\n",
    "        if not action:\n",
    "            action = choose_action(state)\n",
    "        next_state, reward, terminate, truncated, info = env.step(action)\n",
    "        if not terminate:\n",
    "            next_action = choose_action(next_state)\n",
    "            agent_step()\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        else:\n",
    "            agent_end()\n",
    "        total_rewards.append(reward)\n",
    "\n",
    "    if episode%10 == 0:\n",
    "        average_rewards.append(sum(total_rewards)/len(total_rewards))\n",
    "#         print(f\"average reward for batch {episode}: {sum(total_rewards)/len(total_rewards)}\")\n",
    "get_optimal_policy()        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot average rewards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot average rewards for each episode batch \\\n",
    "Observe that average reward is increasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(average_rewards)\n",
    "plt.xlabel(\"Episode Batch\")\n",
    "plt.ylabel(\"Average \\n Rewards\",rotation=0, labelpad=20)\n",
    "plt.ylim(round(min(average_rewards)),round(max(average_rewards),4))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our optimal_policy gives the shortest path from S to G \\\n",
    "Beginning with our starting position[index 36], we observe it's optimal action to be 0[UP] \\\n",
    "Subsequent succeeding states[indices 24-34] give the same optimal action as 1[RIGHT] \\\n",
    "Finally, optimal_policy[35] moves agent downwards to the goal state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.]))"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy[36], optimal_policy[24:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our q_values array \\\n",
    "Observe that rows 37-47 are all zeros \\\n",
    "This is because rows 37-46 correspond to cliff-states while state 47 is the goal state \\\n",
    "So naturally, there are no actions taken from these states & therefore, their action-values are not updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.98284463,  -6.8622783 ,  -6.92926874,  -7.01631226],\n",
       "       [ -6.76466455,  -6.70935301,  -6.69029264,  -6.78315817],\n",
       "       [ -6.41514078,  -6.44182674,  -6.4839398 ,  -6.6594808 ],\n",
       "       [ -6.22646397,  -6.15785874,  -6.23733149,  -6.27330435],\n",
       "       [ -5.81879665,  -5.87161841,  -5.8417512 ,  -6.03037583],\n",
       "       [ -5.59873331,  -5.55850042,  -5.55969956,  -5.53938599],\n",
       "       [ -5.3670877 ,  -5.19092124,  -5.14353929,  -5.46804717],\n",
       "       [ -4.86657917,  -4.77248843,  -4.85969263,  -5.02317775],\n",
       "       [ -4.31199908,  -4.26081114,  -4.42077105,  -4.63618792],\n",
       "       [ -4.01263061,  -3.74604801,  -3.86122722,  -3.82761336],\n",
       "       [ -3.36579569,  -3.25681084,  -3.19016403,  -3.57494081],\n",
       "       [ -2.64908109,  -2.82591469,  -2.66556548,  -3.08389385],\n",
       "       [ -6.97841875,  -6.95128253,  -7.09004267,  -7.0817672 ],\n",
       "       [ -6.72193906,  -6.72660033,  -6.9123949 ,  -6.90043202],\n",
       "       [ -6.46941684,  -6.48081287,  -6.48933766,  -6.49877874],\n",
       "       [ -6.22546953,  -6.18373563,  -6.23534353,  -6.3112884 ],\n",
       "       [ -5.82731165,  -5.82809315,  -5.93039542,  -6.14994822],\n",
       "       [ -5.5477665 ,  -5.43397251,  -5.45827603,  -5.42949497],\n",
       "       [ -5.08714058,  -5.01546644,  -5.07462669,  -5.28318873],\n",
       "       [ -4.74975747,  -4.54909283,  -4.60733788,  -4.65092183],\n",
       "       [ -4.23864017,  -4.01935016,  -4.03130363,  -4.51771588],\n",
       "       [ -3.48929961,  -3.39560047,  -3.41509033,  -3.74515188],\n",
       "       [ -2.73428719,  -2.69881505,  -2.68962432,  -2.89889475],\n",
       "       [ -2.04784688,  -2.16914063,  -1.89986232,  -2.3249625 ],\n",
       "       [ -7.17853768,  -7.17570464,  -7.18297904,  -7.18347868],\n",
       "       [ -6.8815259 ,  -6.86189404, -50.3375    ,  -7.0165485 ],\n",
       "       [ -6.58967714,  -6.5132156 , -50.6496875 ,  -6.81849812],\n",
       "       [ -6.17767256,  -6.12579511, -50.545625  ,  -6.31242825],\n",
       "       [ -5.77094443,  -5.6953279 , -50.523125  ,  -5.80074316],\n",
       "       [ -5.23785601,  -5.217031  , -50.8611875 ,  -5.38048592],\n",
       "       [ -4.73984823,  -4.68559   , -50.94795312,  -4.76128069],\n",
       "       [ -4.3951243 ,  -4.0951    , -50.75782813,  -4.54863496],\n",
       "       [ -3.67342629,  -3.439     , -51.14173965,  -3.78711346],\n",
       "       [ -3.19948387,  -2.71      , -51.06332363,  -2.91877712],\n",
       "       [ -2.05037812,  -1.9       , -51.40855403,  -2.35269375],\n",
       "       [ -1.30125   ,  -1.31375   ,  -1.        ,  -1.7548625 ],\n",
       "       [ -7.45813417, -75.225     ,  -7.49159162,  -7.59492321],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rendering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see our agent in action! \\\n",
    "By looping through each state in optimal_policy, we can get the optimal action in that state \\\n",
    "We then pass that action to env.step() before rendering environment again "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x marks agent's position \\\n",
    "c marks cliff states \\\n",
    "T marks terminal state \n",
    "\n",
    "It's interesting to see our agent making it's way to the goal state via the shortest path[hugging the top of the cliff states] using only 100 episodes of training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CliffWalking-v0', new_step_api=True, render_mode='human')\n",
    "state = env.reset()\n",
    "env.render()\n",
    "for episode in range(100):\n",
    "    next_state, reward, done, truncated, info = env.step(int(optimal_policy[state]))\n",
    "    state = next_state\n",
    "    env.render()\n",
    "    \n",
    "    if done:  \n",
    "        break\n",
    "        \n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, Sarsa is able to solve this environment efficiently by considering the sum of reward & next state's action value to be the update target at every timestep. Instead of waiting till the end of each episode before updating, Sarsa makes use of the bootstrap idea of Dynamic Programming & combines it with the sampling idea from monte-carlo to form an efficient algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try Q-learning \\\n",
    "The only difference lies in the agent_step() \\\n",
    "Instead of using the discounted value of the next state-action pair, we assume the agent takes an action that maximizes this value \\\n",
    "Since the agent only uses it's e-greedy policy for getting to the next state, but assumes a greedy policy for estimating q*, Q-learning is considered to be off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0', new_step_api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = np.zeros((48,4))\n",
    "optimal_policy = np.zeros(37)\n",
    "epsilon = 0.1\n",
    "discount = 0.9\n",
    "step_size = 0.5\n",
    "episodes = 100\n",
    "total_rewards = []\n",
    "average_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is agent_step() where we use max(q_values[next_state]) instead of q_values[next_state][next_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(s):\n",
    "    action_prob = np.random.uniform(s)\n",
    "    if action_prob < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.random.choice([idx for idx in range(4) if q_values[s][idx] == max(q_values[s])])\n",
    "    return int(action)    \n",
    "    \n",
    "def get_optimal_policy():\n",
    "    for i in range(len(optimal_policy)):\n",
    "        optimal_policy[i] = np.random.choice([idx for idx in range(4) if q_values[i][idx] == max(q_values[i])])\n",
    "        \n",
    "def agent_step():\n",
    "    best_q = np.random.choice([q for q in q_values[next_state] if q == max(q_values[next_state])])\n",
    "    q_values[state][action] += step_size*(reward + discount*best_q - q_values[state][action])\n",
    "    \n",
    "def agent_end():\n",
    "    q_values[state][action] += step_size*(reward - q_values[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 718.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(episodes)):\n",
    "    state = env.reset()\n",
    "    terminate = False\n",
    "    while not terminate:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, terminate, truncated, info = env.step(action)\n",
    "        if not terminate:\n",
    "            agent_step()\n",
    "            state = next_state\n",
    "        else:\n",
    "            agent_end()\n",
    "        total_rewards.append(reward)\n",
    "\n",
    "    if episode%10 == 0:\n",
    "        average_rewards.append(sum(total_rewards)/len(total_rewards))\n",
    "#         print(f\"average reward for batch {episode}: {sum(total_rewards)/len(total_rewards)}\")\n",
    "get_optimal_policy()        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot average rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(average_rewards)\n",
    "plt.xlabel(\"Episode Batch\")\n",
    "plt.ylabel(\"Average \\n Rewards\",rotation=0, labelpad=20)\n",
    "plt.ylim(round(min(average_rewards)),round(max(average_rewards),4))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that Q-learning agent is also able to find the same optimal path to the goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.]))"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy[36], optimal_policy[24:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, rows 37-47 consist of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.89410964,  -6.84226036,  -6.80415752,  -6.93320904],\n",
       "       [ -6.76466455,  -6.68185913,  -6.74107327,  -6.68718177],\n",
       "       [ -6.41514078,  -6.45903239,  -6.54480943,  -6.59365568],\n",
       "       [ -6.22646397,  -6.18968226,  -6.35497838,  -6.16634351],\n",
       "       [ -6.02785682,  -5.91830658,  -6.00453241,  -5.97659562],\n",
       "       [ -5.59873331,  -5.58063198,  -5.66047277,  -5.78074395],\n",
       "       [ -5.3670877 ,  -5.21209497,  -5.28804156,  -5.50162887],\n",
       "       [ -4.86657917,  -4.7986147 ,  -4.79457506,  -4.88825837],\n",
       "       [ -4.31199908,  -4.28015226,  -4.45054653,  -4.41699266],\n",
       "       [ -4.01263061,  -3.82029414,  -3.75623186,  -4.03640884],\n",
       "       [ -3.36579569,  -3.29730237,  -3.28450687,  -3.59236272],\n",
       "       [ -3.01662704,  -3.01662704,  -2.6844207 ,  -2.7541193 ],\n",
       "       [ -6.96394495,  -7.01381867,  -7.09766019,  -7.08010976],\n",
       "       [ -6.7734418 ,  -6.79870044,  -6.94908663,  -6.91176901],\n",
       "       [ -6.53909239,  -6.53951088,  -6.59546835,  -6.64970923],\n",
       "       [ -6.34053028,  -6.22432228,  -6.21803885,  -6.23201165],\n",
       "       [ -6.00159059,  -5.88219197,  -5.9525255 ,  -6.15710826],\n",
       "       [ -5.73465366,  -5.47678131,  -5.50372824,  -5.57488405],\n",
       "       [ -5.1965909 ,  -5.05448198,  -5.02493303,  -5.39566444],\n",
       "       [ -4.79284665,  -4.5723776 ,  -4.56970852,  -4.67421646],\n",
       "       [ -4.16733106,  -4.04162178,  -4.05171119,  -4.21851781],\n",
       "       [ -3.50022699,  -3.40889217,  -3.39437537,  -4.00026421],\n",
       "       [ -2.83437492,  -2.70056841,  -2.70142804,  -3.13676606],\n",
       "       [ -2.04784688,  -2.26219063,  -1.89993718,  -2.0986125 ],\n",
       "       [ -7.19905983,  -7.17570464,  -7.42253388,  -7.22610427],\n",
       "       [ -6.95407385,  -6.86189404, -50.39375   ,  -6.97651488],\n",
       "       [ -6.59038887,  -6.5132156 , -50.6418125 ,  -6.51722782],\n",
       "       [ -6.26117584,  -6.12579511, -50.225     ,  -6.45377585],\n",
       "       [ -5.77168535,  -5.6953279 , -50.5878125 ,  -5.94532866],\n",
       "       [ -5.33903682,  -5.217031  , -50.83472187,  -5.48539642],\n",
       "       [ -4.90068605,  -4.68559   , -50.94334766,  -4.97104193],\n",
       "       [ -4.18463466,  -4.0951    , -51.16555361,  -4.25245201],\n",
       "       [ -3.45752909,  -3.439     , -51.01798578,  -4.0105604 ],\n",
       "       [ -3.02862151,  -2.71      , -51.19208649,  -3.13942529],\n",
       "       [ -2.24676164,  -1.9       , -51.35748217,  -1.94167016],\n",
       "       [ -1.18875   ,  -1.42625   ,  -1.        ,  -1.295625  ],\n",
       "       [ -7.45813417, -50.        ,  -7.4965591 ,  -7.4965591 ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rendering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CliffWalking-v0', new_step_api=True, render_mode='human')\n",
    "state = env.reset()\n",
    "env.render()\n",
    "for episode in range(100):\n",
    "    next_state, reward, done, truncated, info = env.step(int(optimal_policy[state]))\n",
    "    state = next_state\n",
    "    env.render()\n",
    "    \n",
    "    if done:  \n",
    "        break\n",
    "        \n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that Q-learning is also able to solve this environment efficiently. By taking max(q_values[next_state]), Q-learning simplifies the code a little and yet is able to achieve similar results to Sarsa. \n",
    "\n",
    "Some differences are as followed: \\\n",
    "1) Q-learning directly learns optimal policy while Sarsa learns near-optimal policy. Parameter adjustments may be required for Sarsa to learn an optimal policy \\\n",
    "2) Q-learning follows an e-greedy policy but uses a greedy policy for estimating action values. Sarsa uses the same policy for estimating action values "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
